"""
å¢å¼ºç‰ˆSEOåˆ†æå™¨
é›†æˆæ•°æ®æ”¶é›†å’ŒAIåˆ†æåŠŸèƒ½
"""
import asyncio
import json
import time
from datetime import datetime
from typing import Dict, Any, List, Optional

from seo_collector import SEODataCollector
from seo_agents import SEOAgentOrchestrator


class EnhancedSEOAnalyzer:
    """å¢å¼ºç‰ˆSEOåˆ†æå™¨"""
    
    def __init__(self, use_ai=True):
        self.use_ai = use_ai
        self.collector = SEODataCollector()
        self.orchestrator = SEOAgentOrchestrator() if use_ai else None
    
    async def analyze_website(self, url: str) -> Dict[str, Any]:
        """åˆ†æç½‘ç«™SEO"""
        try:
            print(f"\nğŸš€ å¼€å§‹AI SEOåˆ†æ: {url}")
            start_time = time.time()
            
            # æ•°æ®æ”¶é›†é˜¶æ®µ
            async with self.collector as collector:
                seo_data = await collector.collect_all_data(url)
            
            # å¦‚æœæ²¡æœ‰å¯ç”¨AIï¼Œè¿”å›åŸºç¡€æ•°æ®
            if not self.use_ai or not self.orchestrator:
                return self._generate_basic_report(seo_data)
            
            # AIåˆ†æé˜¶æ®µ
            ai_result = await self.orchestrator.run_full_analysis(seo_data)
            
            # è®¡ç®—æ€»è€—æ—¶
            analysis_time = round(time.time() - start_time, 2)
            ai_result['analysis_time'] = analysis_time
            
            print(f"âœ… AI SEOåˆ†æå®Œæˆï¼è€—æ—¶: {analysis_time}ç§’")
            return ai_result
            
        except Exception as e:
            print(f"âŒ åˆ†æè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            return {"error": str(e)}
    
    def _generate_basic_report(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """ç”ŸæˆåŸºç¡€åˆ†ææŠ¥å‘Šï¼ˆæ— AIï¼‰"""
        print("âš ï¸ ä½¿ç”¨åŸºç¡€åˆ†ææ¨¡å¼")
        
        # è®¡ç®—åŸºç¡€è¯„åˆ†
        scores = self._calculate_basic_scores(data)
        
        return {
            "url": data.get('url'),
            "timestamp": datetime.now().isoformat(),
            "mode": "basic",
            "scores": scores,
            "overall_score": sum(scores.values()) // len(scores) if scores else 0,
            "data": data,
            "recommendations": self._generate_basic_recommendations(data),
            "analysis_time": 0
        }
    
    def _calculate_basic_scores(self, data: Dict[str, Any]) -> Dict[str, int]:
        """è®¡ç®—åŸºç¡€è¯„åˆ†"""
        scores = {
            'technical': 0,
            'content': 0,
            'performance': 0
        }
        
        # æŠ€æœ¯SEOè¯„åˆ† (40%)
        tech_data = data.get('technical_seo', {})
        tech_score = 0
        
        # æ£€æŸ¥åŸºç¡€æŠ€æœ¯è¦ç´ 
        if tech_data.get('robots_txt', {}).get('exists'):
            tech_score += 20
        if tech_data.get('sitemap', {}).get('exists'):
            tech_score += 20
        if tech_data.get('canonical'):
            tech_score += 20
        if tech_data.get('meta_tags', {}).get('viewport'):
            tech_score += 20
        if tech_data.get('html_structure', {}).get('has_doctype'):
            tech_score += 20
        
        scores['technical'] = min(100, tech_score)
        
        # å†…å®¹è´¨é‡è¯„åˆ† (40%)
        content_data = data.get('content_analysis', {})
        content_score = 0
        
        # TDKåˆ†æ
        tdk = content_data.get('tdk', {})
        if 30 <= tdk.get('title_length', 0) <= 60:
            content_score += 25
        if 100 <= tdk.get('description_length', 0) <= 160:
            content_score += 25
        
        # å›¾ç‰‡ä¼˜åŒ–
        images = content_data.get('images', {})
        if images.get('total', 0) > 0:
            alt_ratio = images.get('with_alt', 0) / images.get('total', 1)
            content_score += int(alt_ratio * 50)
        
        scores['content'] = min(100, content_score)
        
        # æ€§èƒ½è¯„åˆ† (20%)
        perf_data = data.get('performance', {})
        load_time = perf_data.get('page_load_time', 0)
        
        if load_time < 2:
            perf_score = 100
        elif load_time < 4:
            perf_score = 80
        elif load_time < 6:
            perf_score = 60
        else:
            perf_score = 40
        
        scores['performance'] = perf_score
        
        return scores
    
    def _generate_basic_recommendations(self, data: Dict[str, Any]) -> List[str]:
        """ç”ŸæˆåŸºç¡€å»ºè®®"""
        recommendations = []
        
        # æŠ€æœ¯å»ºè®®
        tech_data = data.get('technical_seo', {})
        if not tech_data.get('robots_txt', {}).get('exists'):
            recommendations.append("æ·»åŠ robots.txtæ–‡ä»¶ä»¥æŒ‡å¯¼æœç´¢å¼•æ“çˆ¬å–")
        
        if not tech_data.get('sitemap', {}).get('exists'):
            recommendations.append("åˆ›å»ºsitemap.xmlä»¥å¸®åŠ©æœç´¢å¼•æ“ç´¢å¼•ç½‘ç«™")
        
        if not tech_data.get('canonical'):
            recommendations.append("æ·»åŠ canonicalæ ‡ç­¾é¿å…é‡å¤å†…å®¹é—®é¢˜")
        
        # å†…å®¹å»ºè®®
        content_data = data.get('content_analysis', {})
        tdk = content_data.get('tdk', {})
        
        if tdk.get('title_length', 0) < 30 or tdk.get('title_length', 0) > 60:
            recommendations.append("ä¼˜åŒ–æ ‡é¢˜é•¿åº¦ï¼Œå»ºè®®åœ¨30-60å­—ç¬¦ä¹‹é—´")
        
        if tdk.get('description_length', 0) < 100 or tdk.get('description_length', 0) > 160:
            recommendations.append("ä¼˜åŒ–æè¿°é•¿åº¦ï¼Œå»ºè®®åœ¨100-160å­—ç¬¦ä¹‹é—´")
        
        # å›¾ç‰‡å»ºè®®
        images = content_data.get('images', {})
        if images.get('without_alt', 0) > 0:
            recommendations.append(f"ä¸º{images.get('without_alt')}å¼ å›¾ç‰‡æ·»åŠ ALTå±æ€§")
        
        # æ€§èƒ½å»ºè®®
        perf_data = data.get('performance', {})
        load_time = perf_data.get('page_load_time', 0)
        
        if load_time > 3:
            recommendations.append("ä¼˜åŒ–é¡µé¢åŠ è½½é€Ÿåº¦ï¼Œå»ºè®®å‹ç¼©å›¾ç‰‡å’Œå¯ç”¨ç¼“å­˜")
        
        return recommendations


class BatchSEOAnalyzer:
    """æ‰¹é‡SEOåˆ†æå™¨"""
    
    def __init__(self, use_ai=True):
        self.analyzer = EnhancedSEOAnalyzer(use_ai=use_ai)
    
    async def analyze_multiple(self, urls: List[str]) -> List[Dict[str, Any]]:
        """æ‰¹é‡åˆ†æå¤šä¸ªç½‘ç«™"""
        print(f"\nğŸš€ å¼€å§‹æ‰¹é‡åˆ†æ {len(urls)} ä¸ªç½‘ç«™...")
        
        results = []
        
        for i, url in enumerate(urls, 1):
            print(f"\n[{i}/{len(urls)}] åˆ†æ: {url}")
            
            try:
                result = await self.analyzer.analyze_website(url)
                results.append({
                    "url": url,
                    "status": "success" if "error" not in result else "error",
                    "result": result if "error" not in result else None,
                    "error": result.get("error") if "error" in result else None,
                    "timestamp": datetime.now().isoformat()
                })
            except Exception as e:
                print(f"âŒ åˆ†æå¤±è´¥: {url} - {e}")
                results.append({
                    "url": url,
                    "status": "error",
                    "result": None,
                    "error": str(e),
                    "timestamp": datetime.now().isoformat()
                })
        
        success_count = sum(1 for r in results if r['status'] == 'success')
        print(f"\nâœ… æ‰¹é‡åˆ†æå®Œæˆï¼æˆåŠŸ: {success_count}/{len(urls)}")
        
        return results


def batch_analyze_urls(urls: List[str], use_ai=True) -> List[Dict[str, Any]]:
    """
    åŒæ­¥æ‰¹é‡åˆ†æURLæ¥å£
    ä¸ºäº†å…¼å®¹ç°æœ‰çš„è°ƒç”¨æ–¹å¼
    """
    analyzer = BatchSEOAnalyzer(use_ai=use_ai)
    
    # è¿è¡Œå¼‚æ­¥åˆ†æ
    try:
        # åˆ›å»ºæ–°çš„äº‹ä»¶å¾ªç¯ï¼ˆå¦‚æœå½“å‰æ²¡æœ‰è¿è¡Œçš„å¾ªç¯ï¼‰
        try:
            loop = asyncio.get_running_loop()
            # å¦‚æœå·²ç»æœ‰è¿è¡Œçš„äº‹ä»¶å¾ªç¯ï¼Œä½¿ç”¨ run_in_executor
            import concurrent.futures
            with concurrent.futures.ThreadPoolExecutor() as executor:
                future = executor.submit(asyncio.run, analyzer.analyze_multiple(urls))
                return future.result()
        except RuntimeError:
            # æ²¡æœ‰è¿è¡Œçš„äº‹ä»¶å¾ªç¯ï¼Œç›´æ¥è¿è¡Œ
            return asyncio.run(analyzer.analyze_multiple(urls))
    except Exception as e:
        print(f"âŒ æ‰¹é‡åˆ†æå¤±è´¥: {e}")
        return [{
            "url": url,
            "status": "error",
            "result": None,
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        } for url in urls]