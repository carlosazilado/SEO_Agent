"""
SEOåˆ†æå™¨ - åŸºç¡€ç‰ˆæœ¬ï¼ˆæ— å¯è§†åŒ–ä¾èµ–ï¼‰
ä¸´æ—¶ç”¨äºæµ‹è¯•é¡¹ç›®
"""
import asyncio
import json
import time
from datetime import datetime
from typing import Dict, List, Optional, Any
import aiohttp
from bs4 import BeautifulSoup
import requests
from urllib.parse import urljoin, urlparse
import re
import os
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

class EnhancedSEOAgent:
    """å¢å¼ºå‹SEOåˆ†æå™¨"""
    
    def __init__(self, use_ai=True):
        self.use_ai = use_ai
        self.ai_enabled = False
        self.session = None
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        # åˆå§‹åŒ–AIåˆ†æ
        if self.use_ai:
            self._init_ai_analysis()
    
    def _init_ai_analysis(self):
        """åˆå§‹åŒ–AIåˆ†æåŠŸèƒ½"""
        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„AI API
            api_key = os.getenv('SILICONFLOW_API_KEY')
            if api_key:
                self.ai_enabled = True
                print("âœ… AIåˆ†æåŠŸèƒ½å·²å¯ç”¨")
            else:
                print("âš ï¸ æœªæ‰¾åˆ°AI APIå¯†é’¥ï¼Œå°†ä½¿ç”¨åŸºç¡€åˆ†ææ¨¡å¼")
        except Exception as e:
            print(f"âŒ AIåˆ†æåˆå§‹åŒ–å¤±è´¥: {e}")
            self.ai_enabled = False
    
    async def analyze_website(self, url: str) -> Dict[str, Any]:
        """åˆ†æç½‘ç«™SEO"""
        try:
            print(f"\nğŸ” å¼€å§‹åˆ†æç½‘ç«™: {url}")
            start_time = time.time()
            
            # åˆ›å»ºä¼šè¯
            self.session = aiohttp.ClientSession(headers=self.headers)
            
            # è·å–é¡µé¢å†…å®¹
            content = await self._fetch_content(url)
            if not content:
                return {"error": "æ— æ³•è·å–é¡µé¢å†…å®¹"}
            
            # è§£æHTML
            soup = BeautifulSoup(content, 'html.parser')
            
            # åŸºç¡€SEOåˆ†æ
            result = {
                "url": url,
                "timestamp": datetime.now().isoformat(),
                "content_analysis": self._analyze_content(soup),
                "technical_seo": self._analyze_technical_seo(soup, url),
                "performance": await self._analyze_performance(url),
                "seo_score": 0
            }
            
            # AIæ•°æ®åˆ†æ
            if self.ai_enabled:
                print("ğŸ¤– æ‰§è¡ŒAIæ•°æ®åˆ†æ...")
                await self._ai_data_analysis(result)
            else:
                print("âš ï¸ AIåˆ†ææœªå¯ç”¨ï¼Œä½¿ç”¨åŸºç¡€åˆ†ææ¨¡å¼")
                self._generate_basic_analysis(result)
            
            # è®¡ç®—SEOåˆ†æ•°
            result["seo_score"] = self._calculate_seo_score(result)
            
            # ç”Ÿæˆå»ºè®®
            result["recommendations"] = self._generate_recommendations(result)
            
            # è®¡ç®—æ€»è€—æ—¶
            result["analysis_time"] = round(time.time() - start_time, 2)
            
            print(f"âœ… åˆ†æå®Œæˆï¼è€—æ—¶: {result['analysis_time']}ç§’")
            return result
            
        except Exception as e:
            print(f"âŒ åˆ†æè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            return {"error": str(e)}
        finally:
            if self.session:
                await self.session.close()
    
    async def _fetch_content(self, url: str) -> Optional[str]:
        """è·å–é¡µé¢å†…å®¹"""
        try:
            async with self.session.get(url, timeout=30) as response:
                if response.status == 200:
                    return await response.text()
                else:
                    print(f"âš ï¸ HTTPé”™è¯¯: {response.status}")
                    return None
        except Exception as e:
            print(f"âŒ è·å–é¡µé¢å¤±è´¥: {e}")
            return None
    
    def _analyze_content(self, soup: BeautifulSoup) -> Dict[str, Any]:
        """åˆ†æå†…å®¹è´¨é‡"""
        print("ğŸ“ åˆ†æå†…å®¹è´¨é‡...")
        
        analysis = {
            "title": None,
            "meta_description": None,
            "headings": {"h1": [], "h2": [], "h3": []},
            "word_count": 0,
            "images": {"total": 0, "without_alt": 0},
            "links": {"internal": 0, "external": 0},
            "score": 0
        }
        
        # æ ‡é¢˜åˆ†æ
        title_tag = soup.find('title')
        if title_tag:
            analysis["title"] = {
                "text": title_tag.get_text().strip(),
                "length": len(title_tag.get_text().strip())
            }
        
        # Metaæè¿°
        meta_desc = soup.find('meta', attrs={'name': 'description'})
        if meta_desc:
            analysis["meta_description"] = {
                "text": meta_desc.get('content', '').strip(),
                "length": len(meta_desc.get('content', '').strip())
            }
        
        # æ ‡é¢˜ç»“æ„
        for h in soup.find_all(['h1', 'h2', 'h3']):
            analysis["headings"][h.name.lower()].append(h.get_text().strip())
        
        # å†…å®¹ç»Ÿè®¡
        body_text = soup.get_text()
        analysis["word_count"] = len(body_text.split())
        
        # å›¾ç‰‡åˆ†æ
        images = soup.find_all('img')
        analysis["images"]["total"] = len(images)
        for img in images:
            if not img.get('alt'):
                analysis["images"]["without_alt"] += 1
        
        # é“¾æ¥åˆ†æ
        links = soup.find_all('a', href=True)
        base_url = urlparse(soup.find('base')['href'] if soup.find('base') else '')
        
        for link in links:
            href = link['href']
            if href.startswith('http'):
                if urlparse(href).netloc == base_url.netloc if base_url else True:
                    analysis["links"]["internal"] += 1
                else:
                    analysis["links"]["external"] += 1
            elif href.startswith('/') or not href.startswith(('http', '#')):
                analysis["links"]["internal"] += 1
        
        # è®¡ç®—å†…å®¹åˆ†æ•°
        analysis["score"] = self._calculate_content_score(analysis)
        
        return analysis
    
    def _analyze_technical_seo(self, soup: BeautifulSoup, url: str) -> Dict[str, Any]:
        """åˆ†ææŠ€æœ¯SEO"""
        print("ğŸ”§ åˆ†ææŠ€æœ¯SEO...")
        
        analysis = {
            "meta_robots": None,
            "canonical": None,
            "schema_org": [],
            "open_graph": {},
            "twitter_card": {},
            "mobile_friendly": {"score": 80},  # ç®€åŒ–æ£€æµ‹
            "https": url.startswith('https://'),
            "score": 0
        }
        
        # Meta robots
        meta_robots = soup.find('meta', attrs={'name': 'robots'})
        if meta_robots:
            analysis["meta_robots"] = meta_robots.get('content', '')
        
        # Canonicalæ ‡ç­¾
        canonical = soup.find('link', attrs={'rel': 'canonical'})
        if canonical:
            analysis["canonical"] = canonical.get('href', '')
        
        # ç»“æ„åŒ–æ•°æ®
        scripts = soup.find_all('script', type='application/ld+json')
        for script in scripts:
            try:
                data = json.loads(script.get_text())
                analysis["schema_org"].append(data.get('@type', 'Unknown'))
            except:
                pass
        
        # Open Graph
        for meta in soup.find_all('meta', attrs={'property': lambda x: x and x.startswith('og:')}):
            prop = meta.get('property')[3:]
            analysis["open_graph"][prop] = meta.get('content', '')
        
        # Twitter Card
        for meta in soup.find_all('meta', attrs={'name': lambda x: x and x.startswith('twitter:')}):
            prop = meta.get('name')[8:]
            analysis["twitter_card"][prop] = meta.get('content', '')
        
        # è®¡ç®—æŠ€æœ¯SEOåˆ†æ•°
        analysis["score"] = self._calculate_technical_score(analysis)
        
        return analysis
    
    async def _analyze_performance(self, url: str) -> Dict[str, Any]:
        """åˆ†ææ€§èƒ½æŒ‡æ ‡ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        print("âš¡ åˆ†ææ€§èƒ½æŒ‡æ ‡...")
        
        try:
            start = time.time()
            response = requests.get(url, timeout=10)
            load_time = (time.time() - start) * 1000
            
            return {
                "load_time": round(load_time, 2),
                "status_code": response.status_code,
                "size": len(response.content),
                "score": max(0, 100 - (load_time / 10))  # ç®€å•è®¡ç®—
            }
        except:
            return {
                "load_time": 0,
                "status_code": 0,
                "size": 0,
                "score": 0
            }
    
    def _calculate_content_score(self, content: Dict[str, Any]) -> int:
        """è®¡ç®—å†…å®¹è´¨é‡åˆ†æ•°"""
        score = 0
        
        # æ ‡é¢˜ (30åˆ†)
        if content["title"]:
            score += 15
            if 30 <= content["title"]["length"] <= 60:
                score += 15
        
        # Metaæè¿° (20åˆ†)
        if content["meta_description"]:
            score += 10
            if 120 <= content["meta_description"]["length"] <= 160:
                score += 10
        
        # æ ‡é¢˜ç»“æ„ (20åˆ†)
        if content["headings"]["h1"]:
            score += 10
            if len(content["headings"]["h1"]) == 1:
                score += 5
        
        # å†…å®¹é•¿åº¦ (15åˆ†)
        if content["word_count"] >= 300:
            score += 15
        elif content["word_count"] >= 100:
            score += 8
        
        # å›¾ç‰‡ALT (10åˆ†)
        if content["images"]["total"] > 0:
            alt_ratio = 1 - (content["images"]["without_alt"] / content["images"]["total"])
            score += int(alt_ratio * 10)
        
        # é“¾æ¥ (5åˆ†)
        if content["links"]["internal"] > 0:
            score += 5
        
        return score
    
    def _calculate_technical_score(self, tech: Dict[str, Any]) -> int:
        """è®¡ç®—æŠ€æœ¯SEOåˆ†æ•°"""
        score = 0
        
        # HTTPS (20åˆ†)
        if tech["https"]:
            score += 20
        
        # Meta robots (10åˆ†)
        if tech["meta_robots"]:
            score += 10
        
        # Canonical (10åˆ†)
        if tech["canonical"]:
            score += 10
        
        # ç»“æ„åŒ–æ•°æ® (20åˆ†)
        if tech["schema_org"]:
            score += 20
        
        # Open Graph (20åˆ†)
        if tech["open_graph"]:
            score += 20
        
        # ç§»åŠ¨å‹å¥½æ€§ (20åˆ†)
        score += tech["mobile_friendly"]["score"] * 0.2
        
        return int(score)
    
    def _calculate_seo_score(self, result: Dict[str, Any]) -> int:
        """è®¡ç®—æ€»ä½“SEOåˆ†æ•°"""
        content_weight = 0.4
        technical_weight = 0.3
        performance_weight = 0.3
        
        content_score = result["content_analysis"]["score"]
        technical_score = result["technical_seo"]["score"]
        performance_score = result["performance"]["score"]
        
        total_score = (
            content_score * content_weight +
            technical_score * technical_weight +
            performance_score * performance_weight
        )
        
        return round(total_score)
    
    def _generate_recommendations(self, result: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        recommendations = []
        
        # å†…å®¹å»ºè®®
        if not result["content_analysis"]["title"]:
            recommendations.append({
                "issue": "ç¼ºå°‘é¡µé¢æ ‡é¢˜",
                "solution": "æ·»åŠ æè¿°æ€§çš„é¡µé¢æ ‡é¢˜ï¼ŒåŒ…å«å…³é”®è¯",
                "priority": "high"
            })
        
        if result["content_analysis"]["title"] and result["content_analysis"]["title"]["length"] > 60:
            recommendations.append({
                "issue": "æ ‡é¢˜è¿‡é•¿",
                "solution": "å°†æ ‡é¢˜ç¼©çŸ­è‡³60ä¸ªå­—ç¬¦ä»¥å†…",
                "priority": "medium"
            })
        
        if not result["content_analysis"]["meta_description"]:
            recommendations.append({
                "issue": "ç¼ºå°‘Metaæè¿°",
                "solution": "æ·»åŠ å¸å¼•äººçš„Metaæè¿°ï¼Œæé«˜ç‚¹å‡»ç‡",
                "priority": "high"
            })
        
        # æŠ€æœ¯å»ºè®®
        if not result["technical_seo"]["https"]:
            recommendations.append({
                "issue": "æœªä½¿ç”¨HTTPS",
                "solution": "å®‰è£…SSLè¯ä¹¦ï¼Œå¯ç”¨HTTPS",
                "priority": "high"
            })
        
        if not result["technical_seo"]["canonical"]:
            recommendations.append({
                "issue": "ç¼ºå°‘Canonicalæ ‡ç­¾",
                "solution": "æ·»åŠ Canonicalæ ‡ç­¾é˜²æ­¢é‡å¤å†…å®¹",
                "priority": "medium"
            })
        
        # æ€§èƒ½å»ºè®®
        if result["performance"]["load_time"] > 3000:
            recommendations.append({
                "issue": "é¡µé¢åŠ è½½é€Ÿåº¦æ…¢",
                "solution": "ä¼˜åŒ–å›¾ç‰‡ã€å¯ç”¨ç¼“å­˜ã€ä½¿ç”¨CDN",
                "priority": "high"
            })
        
        return recommendations
    
    async def _ai_data_analysis(self, result: Dict[str, Any]):
        """AIæ•°æ®åˆ†æï¼ˆå ä½ç¬¦ï¼‰"""
        # è¿™é‡Œå¯ä»¥é›†æˆå®é™…çš„AIåˆ†æ
        pass
    
    def _generate_basic_analysis(self, result: Dict[str, Any]):
        """ç”ŸæˆåŸºç¡€åˆ†æç»“æœ"""
        # æ·»åŠ ä¸€äº›åŸºç¡€çš„åˆ†ææ•°æ®
        result["ai_insights"] = {
            "content_quality": "è‰¯å¥½",
            "keyword_density": "é€‚ä¸­",
            "readability": "å®¹æ˜“ç†è§£"
        }


class BatchSEOAnalyzer:
    """æ‰¹é‡SEOåˆ†æå™¨"""
    
    def __init__(self, use_ai=True):
        self.agent = EnhancedSEOAgent(use_ai=use_ai)
    
    async def analyze_multiple(self, urls: List[str]) -> List[Dict[str, Any]]:
        """æ‰¹é‡åˆ†æå¤šä¸ªç½‘ç«™"""
        print(f"\nğŸš€ å¼€å§‹æ‰¹é‡åˆ†æ {len(urls)} ä¸ªç½‘ç«™...")
        
        results = []
        for i, url in enumerate(urls, 1):
            print(f"\n[{i}/{len(urls)}] åˆ†æ: {url}")
            result = await self.agent.analyze_website(url)
            results.append({
                "url": url,
                "status": "success" if "error" not in result else "error",
                "result": result if "error" not in result else None,
                "error": result.get("error") if "error" in result else None,
                "timestamp": datetime.now().isoformat()
            })
        
        print(f"\nâœ… æ‰¹é‡åˆ†æå®Œæˆï¼æˆåŠŸ: {sum(1 for r in results if r['status'] == 'success')}")
        return results