"""
SEOæ•°æ®æ”¶é›†å™¨
è´Ÿè´£æ”¶é›†å„ç§SEOç›¸å…³æ•°æ®
"""
import asyncio
import aiohttp
import json
import time
import platform
from datetime import datetime
from typing import Dict, Any, List, Optional
from urllib.parse import urljoin, urlparse
import whois
from playwright.async_api import async_playwright
import re


class SEODataCollector:
    """SEOæ•°æ®æ”¶é›†å™¨ - ä¿®å¤ç‰ˆæœ¬ï¼Œè§£å†³å¹¶å‘é—®é¢˜"""
    
    def __init__(self):
        self.session = None
        self.playwright = None
        self.browser = None
        self.context = None
        self._page_semaphore = None  # é™åˆ¶å¹¶å‘é¡µé¢æ•°é‡
        self._browser_lock = None  # æµè§ˆå™¨æ“ä½œé”
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
    
    async def __aenter__(self):
        self.session = aiohttp.ClientSession(headers=self.headers)
        self._browser_lock = asyncio.Lock()
        
        # å°è¯•åˆå§‹åŒ– Playwright (Windows å…¼å®¹æ€§æ”¹è¿›)
        try:
            # è®¾ç½®Windowsäº‹ä»¶å¾ªç¯ç­–ç•¥
            if platform.system() == 'Windows':
                try:
                    asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())
                except AttributeError:
                    asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
            
            self.playwright = await async_playwright().start()
            
            # å°è¯•å¯åŠ¨æµè§ˆå™¨ï¼Œä½¿ç”¨å¤šç§é…ç½®
            browser_configs = [
                # é…ç½®1ï¼šç¨³å®šé…ç½®
                {
                    'headless': True,
                    'args': [
                        '--no-sandbox',
                        '--disable-setuid-sandbox', 
                        '--disable-dev-shm-usage',
                        '--disable-web-security',
                        '--disable-gpu',
                        '--disable-software-rasterizer',
                        '--disable-background-timer-throttling',
                        '--disable-renderer-backgrounding',
                        '--disable-backgrounding-occluded-windows'
                    ]
                },
                # é…ç½®2ï¼šç®€åŒ–é…ç½®
                {
                    'headless': True,
                    'args': ['--no-sandbox', '--disable-gpu', '--disable-dev-shm-usage']
                },
                # é…ç½®3ï¼šé»˜è®¤é…ç½®
                {
                    'headless': True
                }
            ]
            
            for i, config in enumerate(browser_configs):
                try:
                    print(f"å°è¯•æµè§ˆå™¨é…ç½® {i+1}/3...")
                    self.browser = await self.playwright.chromium.launch(**config)
                    
                    # åˆ›å»ºä¸€ä¸ªæŒä¹…çš„æµè§ˆå™¨ä¸Šä¸‹æ–‡ï¼Œé…ç½®æ›´å¤šé€‰é¡¹
                    self.context = await self.browser.new_context(
                        user_agent=self.headers['User-Agent'],
                        viewport={'width': 1920, 'height': 1080},
                        ignore_https_errors=True,
                        java_script_enabled=True
                    )
                    
                    # è®¾ç½®é»˜è®¤è¶…æ—¶
                    self.context.set_default_timeout(30000)
                    self.context.set_default_navigation_timeout(30000)
                    
                    # åˆ›å»ºä¿¡å·é‡é™åˆ¶å¹¶å‘é¡µé¢æ•°é‡ - åªå…è®¸1ä¸ªé¡µé¢åŒæ—¶æ“ä½œ
                    self._page_semaphore = asyncio.Semaphore(1)
                    print("âœ… æµè§ˆå™¨å¯åŠ¨æˆåŠŸ")
                    break
                except Exception as e:
                    print(f"é…ç½® {i+1} å¤±è´¥: {e}")
                    if self.browser:
                        try:
                            await self.browser.close()
                        except:
                            pass
                    if i == len(browser_configs) - 1:
                        raise e
                        
        except Exception as e:
            print(f"âš ï¸ Playwrightåˆå§‹åŒ–å¤±è´¥: {e}")
            print("âš ï¸ æµè§ˆå™¨ä¸å¯ç”¨ï¼Œä½¿ç”¨ç®€åŒ–æ•°æ®æ”¶é›†æ¨¡å¼")
            self.playwright = None
            self.browser = None
            self.context = None
        
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """æ¸…ç†èµ„æº"""
        if self.context:
            try:
                await self.context.close()
            except:
                pass
        if self.browser:
            try:
                await self.browser.close()
            except:
                pass
        if self.playwright:
            try:
                await self.playwright.stop()
            except:
                pass
        if self.session:
            try:
                await self.session.close()
            except:
                pass
    
    async def _get_page(self):
        """å®‰å…¨åœ°è·å–ä¸€ä¸ªé¡µé¢å®ä¾‹"""
        if not self.context:
            return None
        
        async with self._page_semaphore:
            try:
                async with self._browser_lock:
                    page = await self.context.new_page()
                    return page
            except Exception as e:
                print(f"âš ï¸ åˆ›å»ºé¡µé¢å¤±è´¥: {e}")
                return None
    
    async def _close_page(self, page):
        """å®‰å…¨åœ°å…³é—­é¡µé¢"""
        if page:
            try:
                await page.close()
            except:
                pass
    
    async def collect_all_data(self, url: str) -> Dict[str, Any]:
        """æ”¶é›†æ‰€æœ‰SEOæ•°æ®"""
        print(f"\nğŸ” å¼€å§‹æ”¶é›†SEOæ•°æ®: {url}")
        
        data = {
            'url': url,
            'timestamp': datetime.now().isoformat(),
            'basic_info': {},
            'technical_seo': {},
            'content_analysis': {},
            'performance': {},
            'traffic_data': {},
            'serp_data': {}
        }
        
        # å…ˆä½¿ç”¨ç®€åŒ–æ¨¡å¼ç¡®ä¿åŠŸèƒ½æ­£å¸¸ 
        print("ğŸ”§ ä½¿ç”¨ç®€åŒ–æ•°æ®æ”¶é›†æ¨¡å¼ä»¥ç¡®ä¿ç¨³å®šæ€§")
        data['basic_info'] = await self._get_basic_info(url)
        data['traffic_data'] = await self._get_traffic_data(url)
        data['serp_data'] = await self._get_serp_data(url)
        
        # ä½¿ç”¨HTTPè¯·æ±‚è·å–å†…å®¹åˆ†æ
        content_data = await self._analyze_content_simple(url)
        if content_data:
            data['content_analysis'] = content_data
        
        # å¦‚æœæµè§ˆå™¨å¯ç”¨ï¼Œè·å–æŠ€æœ¯SEOå’Œæ€§èƒ½æ•°æ®
        if self.context:
            try:
                data['technical_seo'] = await self._analyze_technical_seo(url)
                data['performance'] = await self._get_performance_metrics(url)
            except Exception as e:
                print(f"âš ï¸ æµè§ˆå™¨æ•°æ®æ”¶é›†å¤±è´¥: {e}")
        else:
            print("âš ï¸ æµè§ˆå™¨ä¸å¯ç”¨ï¼Œè·³è¿‡æŠ€æœ¯SEOå’Œæ€§èƒ½åˆ†æ")
        
        print("âœ… SEOæ•°æ®æ”¶é›†å®Œæˆ")
        return data
    
    async def _get_basic_info(self, url: str) -> Dict[str, Any]:
        """è·å–åŸºç¡€ä¿¡æ¯"""
        print("ğŸ“ è·å–åŸºç¡€ä¿¡æ¯...")
        
        try:
            domain = urlparse(url).netloc
            w = whois.whois(domain)
            
            return {
                'domain': domain,
                'domain_age': self._calculate_domain_age(w.creation_date),
                'registrar': w.registrar,
                'nameservers': w.name_servers,
                'ssl_info': await self._check_ssl(url)
            }
        except Exception as e:
            print(f"âŒ è·å–åŸºç¡€ä¿¡æ¯å¤±è´¥: {e}")
            return {}
    
    async def _analyze_technical_seo(self, url: str) -> Dict[str, Any]:
        """åˆ†ææŠ€æœ¯SEO"""
        print("ğŸ”§ åˆ†ææŠ€æœ¯SEO...")
        
        if not self.context:
            print("âš ï¸ æµè§ˆå™¨ä¸Šä¸‹æ–‡ä¸å¯ç”¨ï¼Œè·³è¿‡æŠ€æœ¯SEOåˆ†æ")
            return {}
        
        page = await self._get_page()
        if not page:
            print("âš ï¸ æ— æ³•åˆ›å»ºé¡µé¢ï¼Œè·³è¿‡æŠ€æœ¯SEOåˆ†æ")
            return {}
        
        try:
            await page.goto(url, wait_until="networkidle", timeout=30000)
            
            # æ£€æŸ¥robots.txt
            robots_url = urljoin(url, '/robots.txt')
            robots_content = await self._fetch_url_content(robots_url)
            
            # æ£€æŸ¥sitemap.xml
            sitemap_url = urljoin(url, '/sitemap.xml')
            sitemap_content = await self._fetch_url_content(sitemap_url)
            
            # åˆ†æHTMLç»“æ„
            html = await page.content()
            structure_analysis = self._analyze_html_structure(html)
            
            return {
                'robots_txt': {
                    'exists': bool(robots_content),
                    'content': robots_content[:500] if robots_content else None
                },
                'sitemap_xml': {
                    'exists': bool(sitemap_content),
                    'content': sitemap_content[:500] if sitemap_content else None
                },
                'html_structure': structure_analysis,
                'ssl_enabled': url.startswith('https://')
            }
            
        except Exception as e:
            print(f"âš ï¸ æ•°æ®æ”¶é›†å¤±è´¥: {e}")
            return {}
        finally:
            await self._close_page(page)
    
    async def _analyze_content(self, url: str) -> Dict[str, Any]:
        """åˆ†æå†…å®¹è´¨é‡"""
        print("ğŸ“„ åˆ†æå†…å®¹è´¨é‡...")
        
        if not self.context:
            print("âš ï¸ æµè§ˆå™¨ä¸Šä¸‹æ–‡ä¸å¯ç”¨ï¼Œä½¿ç”¨ç®€åŒ–æ¨¡å¼")
            return await self._analyze_content_simple(url)
        
        page = await self._get_page()
        if not page:
            print("âš ï¸ æ— æ³•åˆ›å»ºé¡µé¢ï¼Œä½¿ç”¨ç®€åŒ–æ¨¡å¼")
            return await self._analyze_content_simple(url)
        
        try:
            await page.goto(url, wait_until="domcontentloaded", timeout=15000)
            
            # åˆ†æé¡µé¢å†…å®¹ - ç®€åŒ–JavaScriptæ‰§è¡Œ
            content_data = await page.evaluate('''() => {
                try {
                    const title = document.querySelector('title')?.textContent || '';
                    const description = document.querySelector('meta[name="description"]')?.content || '';
                    const h1s = Array.from(document.querySelectorAll('h1')).map(h => h.textContent);
                    const h2s = Array.from(document.querySelectorAll('h2')).map(h => h.textContent);
                    const images = Array.from(document.querySelectorAll('img'));
                    const links = Array.from(document.querySelectorAll('a[href]'));
                    
                    const wordCount = document.body ? document.body.innerText.split(/\\s+/).filter(w => w.length > 0).length : 0;
                    
                    return {
                        tdk: {
                            title: title,
                            title_length: title.length,
                            description: description,
                            description_length: description.length,
                            keywords: document.querySelector('meta[name="keywords"]')?.content || ''
                        },
                        headings: {
                            h1: h1s,
                            h2: h2s,
                            h1_count: h1s.length,
                            h2_count: h2s.length
                        },
                        images: {
                            total: images.length,
                            without_alt: images.filter(img => !img.alt || img.alt.trim() === '').length,
                            with_alt: images.filter(img => img.alt && img.alt.trim() !== '').length
                        },
                        links: {
                            total: links.length,
                            internal: links.filter(link => {
                                try {
                                    return link.href.includes(window.location.hostname) || link.href.startsWith('/');
                                } catch(e) {
                                    return false;
                                }
                            }).length,
                            external: links.filter(link => {
                                try {
                                    return !link.href.includes(window.location.hostname) && !link.href.startsWith('/') && link.href.startsWith('http');
                                } catch(e) {
                                    return false;
                                }
                            }).length
                        },
                        content_metrics: {
                            word_count: wordCount,
                            reading_time: Math.ceil(wordCount / 200)
                        }
                    };
                } catch(e) {
                    console.error('Content analysis error:', e);
                    return null;
                }
            }()');
            
            if content_data:
                return content_data
            else:
                print("âš ï¸ JavaScriptæ‰§è¡Œå¤±è´¥ï¼Œä½¿ç”¨ç®€åŒ–æ¨¡å¼")
                return await self._analyze_content_simple(url)
                
        except Exception as e:
            print(f"âŒ å†…å®¹åˆ†æå¤±è´¥ï¼Œä½¿ç”¨ç®€åŒ–æ¨¡å¼: {e}")
            return await self._analyze_content_simple(url)
        finally:
            if page:
                try:
                    await page.close()
                except:
                    pass
                
                return {
                    title: title,
                    title_length: title.length,
                    description: description,
                    description_length: description.length,
                    h1_count: h1s.length,
                    h2_count: h2s.length,
                    h1_texts: h1s,
                    word_count: wordCount,
                    image_count: images.length,
                    images_without_alt: images.filter(img => !img.alt).length,
                    internal_links: links.filter(link => 
                        link.href.includes(window.location.hostname) || 
                        link.href.startsWith('/')
                    ).length,
                    external_links: links.filter(link => 
                        !link.href.includes(window.location.hostname) && 
                        !link.href.startsWith('/') &&
                        (link.href.startsWith('http') || link.href.startsWith('https'))
                    ).length
                };
            }''')
            
            return content_data
            
        except Exception as e:
            print(f"âš ï¸ å†…å®¹åˆ†æå¤±è´¥: {e}")
            return {}
        finally:
            await self._close_page(page)
            return {}
        
        async with self._page_semaphore:
            page = None
            try:
                page = await self.context.new_page()
                await page.goto(url, wait_until="networkidle", timeout=30000)
                
                content = await page.evaluate('''
                    () => {
                        const title = document.title;
                        const description = document.querySelector('meta[name="description"]')?.content || '';
                        const keywords = document.querySelector('meta[name="keywords"]')?.content || '';
                        
                        // åˆ†ææ ‡é¢˜ç»“æ„
                        const headings = {};
                        for (let i = 1; i <= 6; i++) {
                            headings[`h${i}`] = Array.from(document.querySelectorAll(`h${i}`)).map(h => h.textContent.trim());
                        }
                        
                        // åˆ†æå›¾ç‰‡
                        const images = Array.from(document.querySelectorAll('img'));
                        const imageAnalysis = {
                            total: images.length,
                            without_alt: images.filter(img => !img.alt || img.alt.trim() === '').length,
                            with_alt: images.filter(img => img.alt && img.alt.trim() !== '').length
                        };
                        
                        // åˆ†æé“¾æ¥
                        const links = Array.from(document.querySelectorAll('a[href]'));
                        const linkAnalysis = {
                            total: links.length,
                            internal: links.filter(link => link.href.includes(window.location.hostname)).length,
                            external: links.filter(link => !link.href.includes(window.location.hostname)).length,
                            nofollow: links.filter(link => link.rel?.includes('nofollow')).length
                        };
                        
                        // OGæ ‡ç­¾
                        const ogTags = {};
                        document.querySelectorAll('meta[property^="og:"]').forEach(tag => {
                            ogTags[tag.getAttribute('property')] = tag.getAttribute('content');
                        });
                        
                        // Twitter Cards
                        const twitterTags = {};
                        document.querySelectorAll('meta[name^="twitter:"]').forEach(tag => {
                            twitterTags[tag.getAttribute('name')] = tag.getAttribute('content');
                        });
                        
                        // æ–‡æœ¬å†…å®¹åˆ†æ
                        const textContent = document.body.innerText;
                        const wordCount = textContent.split(/\\s+/).filter(word => word.length > 0).length;
                        
                        return {
                            tdk: {
                                title: title,
                                title_length: title.length,
                                description: description,
                                description_length: description.length,
                                keywords: keywords
                            },
                            headings: headings,
                            images: imageAnalysis,
                            links: linkAnalysis,
                            social_tags: {
                                og: ogTags,
                                twitter: twitterTags
                            },
                            content_metrics: {
                                word_count: wordCount,
                                reading_time: Math.ceil(wordCount / 200)  // å‡è®¾æ¯åˆ†é’Ÿé˜…è¯»200å­—
                            }
                        };
                    }
                ''')
                
                return content
            except Exception as e:
                print(f"âŒ å†…å®¹åˆ†æå¤±è´¥: {e}")
                return {}
            finally:
                if page:
                    try:
                        await page.close()
                    except:
                        pass
    
    async def _analyze_content_simple(self, url: str) -> Dict[str, Any]:
        """ä½¿ç”¨HTTPè¯·æ±‚åˆ†æå†…å®¹"""
        try:
            async with self.session.get(url) as response:
                if response.status == 200:
                    html = await response.text()
                    if BeautifulSoup:
                        soup = BeautifulSoup(html, 'html.parser')
                        
                        # æå–åŸºæœ¬ä¿¡æ¯
                        title = soup.find('title')
                        title_text = title.text.strip() if title else ''
                        
                        description = soup.find('meta', attrs={'name': 'description'})
                        desc_text = description.get('content', '') if description else ''
                        
                        # åˆ†ææ ‡é¢˜
                        headings = {}
                        for i in range(1, 7):
                            h_tags = soup.find_all(f'h{i}')
                            headings[f'h{i}'] = [h.text.strip() for h in h_tags]
                        
                        # åˆ†æå›¾ç‰‡
                        images = soup.find_all('img')
                        img_no_alt = sum(1 for img in images if not img.get('alt'))
                        
                        # åˆ†æé“¾æ¥
                        links = soup.find_all('a', href=True)
                        domain = urlparse(url).netloc
                        internal_links = sum(1 for link in links if domain in link.get('href', ''))
                        
                        return {
                            'tdk': {
                                'title': title_text,
                                'title_length': len(title_text),
                                'description': desc_text,
                                'description_length': len(desc_text),
                                'keywords': ''
                            },
                            'headings': headings,
                            'images': {
                                'total': len(images),
                                'without_alt': img_no_alt,
                                'with_alt': len(images) - img_no_alt
                            },
                            'links': {
                                'total': len(links),
                                'internal': internal_links,
                                'external': len(links) - internal_links,
                                'nofollow': 0
                            },
                            'social_tags': {'og': {}, 'twitter': {}},
                            'content_metrics': {
                                'word_count': len(soup.get_text().split()),
                                'reading_time': 0
                            }
                        }
        except Exception as e:
            print(f"âŒ ç®€åŒ–å†…å®¹åˆ†æå¤±è´¥: {e}")
        
        return {}
    
    async def _get_performance_metrics(self, url: str) -> Dict[str, Any]:
        """è·å–æ€§èƒ½æŒ‡æ ‡"""
        print("âš¡ è·å–æ€§èƒ½æŒ‡æ ‡...")
        
        if not self.context:
            print("âš ï¸ æµè§ˆå™¨ä¸Šä¸‹æ–‡ä¸å¯ç”¨ï¼Œè·³è¿‡æ€§èƒ½åˆ†æ")
            return {}
        
        page = await self._get_page()
        if not page:
            print("âš ï¸ æ— æ³•åˆ›å»ºé¡µé¢ï¼Œè·³è¿‡æ€§èƒ½åˆ†æ")
            return {}
        
        try:
            start_time = time.time()
            await page.goto(url, wait_until="networkidle", timeout=30000)
            load_time = time.time() - start_time
            
            # è·å–é¡µé¢æ€§èƒ½æŒ‡æ ‡
            performance_data = await page.evaluate('''() => {
                const navigation = performance.getEntriesByType('navigation')[0];
                const resources = performance.getEntriesByType('resource');
                
                return {
                    page_load_time: navigation ? navigation.loadEventEnd - navigation.fetchStart : 0,
                    dom_content_loaded: navigation ? navigation.domContentLoadedEventEnd - navigation.fetchStart : 0,
                    first_byte: navigation ? navigation.responseStart - navigation.fetchStart : 0,
                    resources: {
                        total_requests: resources.length,
                        total_size: resources.reduce((sum, r) => sum + (r.transferSize || 0), 0),
                        images: resources.filter(r => r.initiatorType === 'img').length,
                        scripts: resources.filter(r => r.initiatorType === 'script').length,
                        stylesheets: resources.filter(r => r.initiatorType === 'link').length
                    }
                };
            }''')
            
            # æ·»åŠ æˆ‘ä»¬æµ‹é‡çš„åŠ è½½æ—¶é—´
            performance_data['measured_load_time'] = round(load_time, 2)
            
            return performance_data
            
        except Exception as e:
            print(f"âš ï¸ æ€§èƒ½åˆ†æå¤±è´¥: {e}")
            return {'measured_load_time': 0, 'error': str(e)}
        finally:
            await self._close_page(page)
    
    async def _get_traffic_data(self, url: str) -> Dict[str, Any]:
        """è·å–æµé‡æ•°æ®ï¼ˆæ¨¡æ‹Ÿæ•°æ®ï¼Œé¿å…APIä¾èµ–ï¼‰"""
        print("ğŸ“Š è·å–æµé‡æ•°æ®...")
        
        domain = urlparse(url).netloc
        
        # è¿”å›æ¨¡æ‹Ÿçš„æµé‡æ•°æ®ï¼Œé¿å…ä¾èµ–ä»˜è´¹API
        print("â„¹ï¸  ä½¿ç”¨æ¨¡æ‹Ÿæµé‡æ•°æ®ï¼ˆSimilarWeb APIéœ€è¦ä»˜è´¹å¯†é’¥ï¼‰")
        
        return {
            'estimated_visits': {
                'monthly': 'N/A',
                'note': 'éœ€è¦æµé‡åˆ†æAPI'
            },
            'engagement': {
                'avg_visit_duration': 'N/A',
                'pages_per_visit': 'N/A', 
                'bounce_rate': 'N/A'
            },
            'traffic_sources': {
                'direct': 'N/A',
                'search': 'N/A',
                'social': 'N/A',
                'referrals': 'N/A'
            },
            'top_countries': 'N/A',
            'rankings': {
                'global_rank': 'N/A',
                'country_rank': 'N/A'
            },
            'domain': domain,
            'status': 'mock_data'
        }
    
    async def _get_serp_data(self, url: str) -> Dict[str, Any]:
        """è·å–SERPæ•°æ®ï¼ˆæ¨¡æ‹Ÿï¼‰"""
        print("ğŸ” è·å–SERPæ•°æ®...")
        
        # è¿™é‡Œå¯ä»¥é›†æˆOpenSERPæˆ–å…¶ä»–SERP API
        # ç›®å‰è¿”å›æ¨¡æ‹Ÿæ•°æ®
        return {
            'indexed_pages': 'æœªçŸ¥',
            'backlinks_estimate': 'æœªçŸ¥',
            'domain_authority': 0,
            'keyword_rankings': [],
            'note': 'SERPæ•°æ®éœ€è¦é…ç½®ä¸“é—¨çš„APIæœåŠ¡'
        }
    
    async def _fetch_url_content(self, url: str) -> Optional[str]:
        """è·å–URLå†…å®¹"""
        try:
            async with self.session.get(url, timeout=10) as response:
                if response.status == 200:
                    return await response.text()
        except Exception:
            pass
        return None
    
    def _calculate_domain_age(self, creation_date) -> Optional[int]:
        """è®¡ç®—åŸŸåå¹´é¾„"""
        if not creation_date:
            return None
        
        if isinstance(creation_date, list):
            creation_date = creation_date[0]
        
        if isinstance(creation_date, str):
            try:
                creation_date = datetime.strptime(creation_date, '%Y-%m-%d %H:%M:%S')
            except:
                try:
                    creation_date = datetime.strptime(creation_date, '%Y-%m-%d')
                except:
                    return None
        
        days = (datetime.now() - creation_date).days
        return days
    
    def _analyze_html_structure(self, html: str) -> Dict[str, Any]:
        """åˆ†æHTMLç»“æ„"""
        if not BeautifulSoup:
            return {}
            
        soup = BeautifulSoup(html, 'html.parser')
        
        structure_analysis = {
            'has_doctype': bool(html.startswith('<!DOCTYPE')),
            'semantic_tags': {
                'header': len(soup.find_all('header')),
                'nav': len(soup.find_all('nav')),
                'main': len(soup.find_all('main')),
                'article': len(soup.find_all('article')),
                'section': len(soup.find_all('section')),
                'aside': len(soup.find_all('aside')),
                'footer': len(soup.find_all('footer'))
            },
            'total_images': len(soup.find_all('img')),
            'total_links': len(soup.find_all('a')),
            'has_schema': bool(soup.find_all(attrs={'type': 'application/ld+json'}))
        }
        
        return structure_analysis
    
    async def _check_ssl(self, url: str) -> Dict[str, Any]:
        """æ£€æŸ¥SSLè¯ä¹¦"""
        try:
            async with self.session.get(url, ssl=False) as response:
                return {
                    'enabled': url.startswith('https://'),
                    'valid': True,  # ç®€åŒ–æ£€æŸ¥
                    'issuer': 'Unknown'
                }
        except:
            return {
                'enabled': False,
                'valid': False,
                'issuer': None
            }


# BeautifulSoupå¯¼å…¥
try:
    from bs4 import BeautifulSoup
except ImportError:
    BeautifulSoup = None